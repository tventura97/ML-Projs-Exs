{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input training sets\n",
    "#Each input vector corresponds to an entry in the output vector\n",
    "#Each entry in the output vector is just the first entry of its corresponding input vector\n",
    "inputs = np.array([[0,0,1,0], [1,1,1,0], [1,0,1,1], [0,1,1,1], [0,1,0,1], [1,1,1,1], [0,0,0,0]])\n",
    "real_outputs = np.array([[0,1,1,0,0,1,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perceptron class\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self):\n",
    "        #initialize weights as random values.\n",
    "        #the correct weights will be taught, but the first synapse (corresponding to the first entry of the input layer)\n",
    "        #should ultimately be weighted very highly while the others are very low.\n",
    "        self.synapse_weights = np.random.rand(4,1);\n",
    "    \n",
    "    \n",
    "#Activation function\n",
    "#Could use a step function here but we're going to use back-propagation to train the \n",
    "#perceptron, so we need a differentiable function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    #derivative of sigmoid\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return np.exp(-x)/((1 + np.exp(-x))**2)\n",
    "    #Training function\n",
    "\n",
    "    #inputs, outputs, number of iterations, and learning rate\n",
    "    def train(self, inputs, outputs, its, lr):\n",
    "        delta_weights = np.zeros((4,7))\n",
    "\n",
    "        for iteration in (range(its)):\n",
    "\n",
    "            #Forward Pass\n",
    "            z = np.dot(inputs, self.synapse_weights)\n",
    "            activation = self.sigmoid(z)\n",
    "\n",
    "            #Backward Pass\n",
    "            for i in range(7):\n",
    "                #Our error function. We want to minizize this\n",
    "                #This quantity beeing small is how we know that our model is being\n",
    "                #optimized correctly. We're using supervised learning here since\n",
    "                #We know our expected outputs\n",
    "                cost = (activation[i] - real_outputs[i])**2\n",
    "                cost_prime = 2*(activation[i] - real_outputs[i])\n",
    "\n",
    "                for n in range(4):\n",
    "                    delta_weights[n][i] = cost_prime * inputs[i][n] * self.sigmoid_deriv(z[i])\n",
    "                delta_avg = np.array([np.average(delta_weights, axis=1)]).T\n",
    "                #Adjust our synapse weights\n",
    "                self.synapse_weights = self.synapse_weights - delta_avg*lr\n",
    "            #Train the perceptron\n",
    "    def results(self, inputs):\n",
    "        return self.sigmoid(np.dot(inputs, self.synapse_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "[[0.00022413616151323925], [0.040532764413150756], [0.06575540158706443], [0.9999993842683875], [0.9999999739883572], [0.9999996304299275], [0.999991833691076]]\n",
      "[0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ts_input = np.array([[0,0,1,0],\n",
    "                         [1,1,1,0],\n",
    "                         [1,0,1,1],\n",
    "                         [0,1,1,1],\n",
    "                         [0,1,0,1],\n",
    "                         [1,1,1,1],\n",
    "                         [0,0,0,0]])\n",
    "    ts_output = np.array([[0,1,1,0,0,1,0]]).T\n",
    "    testing_data = np.array([[0,1,1,0],\n",
    "                             [0,0,0,1],\n",
    "                             [0,1,0,0],\n",
    "                             [1,0,0,1],\n",
    "                             [1,0,0,0],\n",
    "                             [1,1,0,0],\n",
    "                             [1,0,1,0]])\n",
    "    lr = 10 # learning rate\n",
    "    steps = 10000\n",
    "    perceptron = Perceptron() # initialize a perceptron\n",
    "    perceptron.train(ts_input, ts_output, steps, lr) # train the perceptron\n",
    "    results = []\n",
    "    for x in (range(len(testing_data))):\n",
    "        run = testing_data[x]\n",
    "        trial = perceptron.results(run)\n",
    "        results.append(trial.tolist())\n",
    "    print(\"results\")\n",
    "    print(results)\n",
    "    print(np.ravel(np.rint(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.46472157]\n",
      " [-2.65379647]\n",
      " [-5.7492362 ]\n",
      " [-3.16426752]]\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.synapse_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As expected, our top synapse weight is very high while the others are very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
